{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744343a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0e8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "URL = \"https://www.huffingtonpost.com/entry/donna-edwards-inequality_us_57455f7fe4b055bb1170b207\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "body = soup.find('section', class_='entry__content-list js-entry-content js-cet-subunit')\n",
    "content = body.find_all('div', class_='primary-cli cli cli-text')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for i in content:\n",
    "    article_text += i.text\n",
    "# print(article_text)   \n",
    "\n",
    "# data[\"article_text\"] = article_text\n",
    "# print(data)\n",
    "# output: json format --> \"body\" : \"Employers posted a million fewer job openings...\"\n",
    "# dictionary gebruiken!\n",
    "# per dictionary (=artikel) de body tekst toevoegen\n",
    "# dode linkjes toevoegen aan nieuwe lijst met headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac3d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Opening JSON file\n",
    "f = open('archive\\Sarcasm_Headlines_Dataset_v2.json')\n",
    "\n",
    "# all data and broken urls:\n",
    "full_data = []\n",
    "broken_urls = []\n",
    "\n",
    "with open(\"archive\\Sarcasm_Headlines_Dataset_v2.json\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        full_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e70f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████▋                                         | 12215/28619 [3:01:21<3:02:09,  1.50it/s]"
     ]
    }
   ],
   "source": [
    "# print(full_data[:10])\n",
    "# output_file = open('archive\\Sarcasm_Headlines_Dataset_with_article_text.json', 'w')\n",
    "words = 120\n",
    "\n",
    "for item in tqdm(full_data):\n",
    "    URL = item['article_link']\n",
    "    \n",
    "    try:\n",
    "        page = requests.get(URL)\n",
    "        page.raise_for_status()\n",
    "    \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        article_text = ''\n",
    "        \n",
    "        if \"huffingtonpost\" in URL:\n",
    "            body = soup.find('section', class_='entry__content-list js-entry-content js-cet-subunit')\n",
    "            content = body.find_all('div', class_='primary-cli cli cli-text')\n",
    "\n",
    "            for i in content:\n",
    "                article_text += i.text\n",
    "                \n",
    "            first_n = article_text.split()[:words]\n",
    "            item[\"article_text\"] = \" \".join(first_n)\n",
    "                \n",
    "        elif \"theonion\" in URL:\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            body = soup.find('p', class_='sc-77igqf-0 bOfvBY')\n",
    "            \n",
    "            article_text = body.text\n",
    "\n",
    "            first_n = article_text.split()[:words]    \n",
    "            item[\"article_text\"] = \" \".join(first_n)\n",
    "            \n",
    "    except:\n",
    "#         print(URL,' not working..')\n",
    "        broken_urls.append(URL)\n",
    "        item[\"article_text\"] = ''\n",
    "#     else:\n",
    "#         print(URL,' working!')\n",
    "# json.dumps(full_data[:10])\n",
    "\n",
    "# print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(full_data[:10])\n",
    "# print(\"Full data: \",len(full_data))\n",
    "# print(\"Broken URL's: \",len(broken_urls))\n",
    "# print(broken_urls)\n",
    "\n",
    "with open('archive\\Sarcasm_Headlines_Dataset_with_article_text.json', 'w') as output:\n",
    "    json.dump(full_data, output, indent=0)\n",
    "\n",
    "# json.dumps(full_data[:10],indent=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c18fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(full_data[1:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
